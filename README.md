# Today I Learned.
### 목차

---

> #### NLP

+ 딥 러닝을 이용한 자연어 처리 입문

  + [자연어 처리(natural language processing) 준비하기](https://github.com/p-chanmin/TIL/blob/main/NLP/01-01.%20%EC%9E%90%EC%97%B0%EC%96%B4%20%EC%B2%98%EB%A6%AC(natural%20language%20processing)%20%EC%A4%80%EB%B9%84%ED%95%98%EA%B8%B0.md)
    + [머신 러닝 워크플로우(Machine Learning Workflow)](https://github.com/p-chanmin/TIL/blob/main/NLP/01-02.%20%EB%A8%B8%EC%8B%A0%20%EB%9F%AC%EB%8B%9D%20%EC%9B%8C%ED%81%AC%ED%94%8C%EB%A1%9C%EC%9A%B0.md)
  + [텍스트 전처리(Text preprocessing)](https://github.com/p-chanmin/TIL/blob/main/NLP/02-00.%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EC%A0%84%EC%B2%98%EB%A6%AC(Text%20preprocessing).md)
    + [토큰화(Tokenization)](https://github.com/p-chanmin/TIL/blob/main/NLP/02-01.%20%ED%86%A0%ED%81%B0%ED%99%94(Tokenization).md)
    + [정제(Cleaning) and 정규화(Normalization)](https://github.com/p-chanmin/TIL/blob/main/NLP/02-02.%20%EC%A0%95%EC%A0%9C(Cleaning)%20and%20%EC%A0%95%EA%B7%9C%ED%99%94(Normalization).md)
    + [어간 추출(Stemming) and 표제어 추출(Lemmatization)](https://github.com/p-chanmin/TIL/blob/main/NLP/02-03.%20%EC%96%B4%EA%B0%84%20%EC%B6%94%EC%B6%9C(Stemming)%20and%20%ED%91%9C%EC%A0%9C%EC%96%B4%20%EC%B6%94%EC%B6%9C(Lemmatization).md)
    + [불용어(Stopword)](https://github.com/p-chanmin/TIL/blob/main/NLP/02-04.%20%EB%B6%88%EC%9A%A9%EC%96%B4(Stopword).md)
    + [정규 표현식(Regular Expression)](https://github.com/p-chanmin/TIL/blob/main/NLP/02-05.%20%EC%A0%95%EA%B7%9C%20%ED%91%9C%ED%98%84%EC%8B%9D(Regular%20Expression).md)
    + [정수 인코딩(Integer Encoding)](https://github.com/p-chanmin/TIL/blob/main/NLP/02-06.%20%EC%A0%95%EC%88%98%20%EC%9D%B8%EC%BD%94%EB%94%A9(Integer%20Encoding).md)
    + [패딩(Padding)](https://github.com/p-chanmin/TIL/blob/main/NLP/02-07.%20%ED%8C%A8%EB%94%A9(Padding).md)
    + [원-핫 인코딩(One-Hot Encoding)](https://github.com/p-chanmin/TIL/blob/main/NLP/02-08.%20%EC%9B%90-%ED%95%AB%20%EC%9D%B8%EC%BD%94%EB%94%A9(One-Hot%20Encoding).md)
    + [데이터의 분리(Splitting Data)](https://github.com/p-chanmin/TIL/blob/main/NLP/02-09.%20%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%9D%98%20%EB%B6%84%EB%A6%AC(Splitting%20Data).md)
    + [한국어 전처리 패키지(Text Preprocessing Tools for Korean Text)](https://github.com/p-chanmin/TIL/blob/main/NLP/02-10.%20%ED%95%9C%EA%B5%AD%EC%96%B4%20%EC%A0%84%EC%B2%98%EB%A6%AC%20%ED%8C%A8%ED%82%A4%EC%A7%80(Text%20Preprocessing%20Tools%20for%20Korean%20Text).md)
  + [언어 모델(Language Model)](https://github.com/p-chanmin/TIL/blob/main/NLP/03-00.%20%EC%96%B8%EC%96%B4%20%EB%AA%A8%EB%8D%B8(Language%20Model).md)
    * [언어 모델(Language Model)이란](https://github.com/p-chanmin/TIL/blob/main/NLP/03-01.%20%EC%96%B8%EC%96%B4%20%EB%AA%A8%EB%8D%B8(Language%20Model)%EC%9D%B4%EB%9E%80.md)
    * [통계적 언어 모델(Statistical Language Model, SLM)](https://github.com/p-chanmin/TIL/blob/main/NLP/03-02.%20%ED%86%B5%EA%B3%84%EC%A0%81%20%EC%96%B8%EC%96%B4%20%EB%AA%A8%EB%8D%B8(Statistical%20Language%20Model%2C%20SLM).md)
    * [N-gram 언어 모델(N-gram Language Model)](https://github.com/p-chanmin/TIL/blob/main/NLP/03-03.%20N-gram%20%EC%96%B8%EC%96%B4%20%EB%AA%A8%EB%8D%B8(N-gram%20Language%20Model).md)
    * [한국어에서의 언어 모델(Language Model for Korean Sentences)](https://github.com/p-chanmin/TIL/blob/main/NLP/03-04.%20%ED%95%9C%EA%B5%AD%EC%96%B4%EC%97%90%EC%84%9C%EC%9D%98%20%EC%96%B8%EC%96%B4%20%EB%AA%A8%EB%8D%B8(Language%20Model%20for%20Korean%20Sentences).md)
    * [펄플렉서티(Perplexity, PPL)](https://github.com/p-chanmin/TIL/blob/main/NLP/03-05.%20%ED%8E%84%ED%94%8C%EB%A0%89%EC%84%9C%ED%8B%B0(Perplexity%2C%20PPL).md)
  + [카운트 기반의 단어 표현(Count based word Representation)](https://github.com/p-chanmin/TIL/blob/main/NLP/04-00.%20%EC%B9%B4%EC%9A%B4%ED%8A%B8%20%EA%B8%B0%EB%B0%98%EC%9D%98%20%EB%8B%A8%EC%96%B4%20%ED%91%9C%ED%98%84(Count%20based%20word%20Representation).md)
    * [다양한 단어의 표현 방법](https://github.com/p-chanmin/TIL/blob/main/NLP/04-01.%20%EB%8B%A4%EC%96%91%ED%95%9C%20%EB%8B%A8%EC%96%B4%EC%9D%98%20%ED%91%9C%ED%98%84%20%EB%B0%A9%EB%B2%95.md)
    * [Bag of Words(BoW)](https://github.com/p-chanmin/TIL/blob/main/NLP/04-02.%20Bag%20of%20Words(BoW).md)
    * [문서 단어 행렬(Document-Term Matrix, DTM)](https://github.com/p-chanmin/TIL/blob/main/NLP/04-03.%20%EB%AC%B8%EC%84%9C%20%EB%8B%A8%EC%96%B4%20%ED%96%89%EB%A0%AC(Document-Term%20Matrix%2C%20DTM).md)
    * [TF-IDF(Term Frequency-Inverse Document Frequency)](https://github.com/p-chanmin/TIL/blob/main/NLP/04-04.%20TF-IDF(Term%20Frequency-Inverse%20Document%20Frequency).md)
  + [벡터의 유사도(Vector Similarity)](https://github.com/p-chanmin/TIL/blob/main/NLP/05-00.%20%EB%B2%A1%ED%84%B0%EC%9D%98%20%EC%9C%A0%EC%82%AC%EB%8F%84(Vector%20Similarity).md)
    * [코사인 유사도(Cosine Similarity)](https://github.com/p-chanmin/TIL/blob/main/NLP/05-01.%20%EC%BD%94%EC%82%AC%EC%9D%B8%20%EC%9C%A0%EC%82%AC%EB%8F%84(Cosine%20Similarity).md)
    * [여러가지 유사도 기법](https://github.com/p-chanmin/TIL/blob/main/NLP/05-02.%20%EC%97%AC%EB%9F%AC%EA%B0%80%EC%A7%80%20%EC%9C%A0%EC%82%AC%EB%8F%84%20%EA%B8%B0%EB%B2%95.md)
  + [머신 러닝(Machine Learning) 개요](https://github.com/p-chanmin/TIL/blob/main/NLP/06-00.%20%EB%A8%B8%EC%8B%A0%20%EB%9F%AC%EB%8B%9D(Machine%20Learning)%20%EA%B0%9C%EC%9A%94.md)
    * [머신 러닝이란(What is Machine Learning?)](https://github.com/p-chanmin/TIL/blob/main/NLP/06-01.%20%EB%A8%B8%EC%8B%A0%20%EB%9F%AC%EB%8B%9D%EC%9D%B4%EB%9E%80(What%20is%20Machine%20Learning).md)
    * [머신 러닝 훑어보기](https://github.com/p-chanmin/TIL/blob/main/NLP/06-02.%20%EB%A8%B8%EC%8B%A0%20%EB%9F%AC%EB%8B%9D%20%ED%9B%91%EC%96%B4%EB%B3%B4%EA%B8%B0.md)
    * [선형 회귀(Linear Regression)](https://github.com/p-chanmin/TIL/blob/main/NLP/06-03.%20%EC%84%A0%ED%98%95%20%ED%9A%8C%EA%B7%80(Linear%20Regression).md)
    * [자동 미분과 선형 회귀 실습](https://github.com/p-chanmin/TIL/blob/main/NLP/06-04.%20%EC%9E%90%EB%8F%99%20%EB%AF%B8%EB%B6%84%EA%B3%BC%20%EC%84%A0%ED%98%95%20%ED%9A%8C%EA%B7%80%20%EC%8B%A4%EC%8A%B5.md)
    * [로지스틱 회귀(Logistic Regression)](https://github.com/p-chanmin/TIL/blob/main/NLP/06-05.%20%EB%A1%9C%EC%A7%80%EC%8A%A4%ED%8B%B1%20%ED%9A%8C%EA%B7%80(Logistic%20Regression).md)
    * [로지스틱 회귀 실습](https://github.com/p-chanmin/TIL/blob/main/NLP/06-06.%20%EB%A1%9C%EC%A7%80%EC%8A%A4%ED%8B%B1%20%ED%9A%8C%EA%B7%80%20%EC%8B%A4%EC%8A%B5.md)
    * [다중 입력에 대한 실습](https://github.com/p-chanmin/TIL/blob/main/NLP/06-07.%20%EB%8B%A4%EC%A4%91%20%EC%9E%85%EB%A0%A5%EC%97%90%20%EB%8C%80%ED%95%9C%20%EC%8B%A4%EC%8A%B5.md)
    * [벡터와 행렬 연산](https://github.com/p-chanmin/TIL/blob/main/NLP/06-08.%20%EB%B2%A1%ED%84%B0%EC%99%80%20%ED%96%89%EB%A0%AC%20%EC%97%B0%EC%82%B0.md)
    * [소프트맥스 회귀(Softmax Regression)](https://github.com/p-chanmin/TIL/blob/main/NLP/06-09.%20%EC%86%8C%ED%94%84%ED%8A%B8%EB%A7%A5%EC%8A%A4%20%ED%9A%8C%EA%B7%80(Softmax%20Regression).md)
    * [소프트맥스 회귀 실습](https://github.com/p-chanmin/TIL/blob/main/NLP/06-10.%20%EC%86%8C%ED%94%84%ED%8A%B8%EB%A7%A5%EC%8A%A4%20%ED%9A%8C%EA%B7%80%20%EC%8B%A4%EC%8A%B5.md)
  + 딥 러닝(Deep Learning) 개요
    * 퍼셉트론(Perceptron)
    * 인공 신경망(Artificial Neural Network) 훑어보기
    * 행렬곱으로 이해하는 신경망
    * 딥 러닝의 학습 방법
    * 역전파(BackPropagation) 이해하기
    * 과적합(Overfitting)을 막는 방법들
    * 기울기 소실(Gradient Vanishing)과 폭주(Exploding)
    * 케라스(Keras) 훑어보기
    * 케라스의 함수형 API(Keras Functional API)
    * 케라스 서브클래싱 API(Keras Subclassing API)
    * 다층 퍼셉트론(MultiLayer Perceptron, MLP)으로 텍스트 분류하기
    * 피드 포워드 신경망 언어 모델(Neural Network Language Model, NNLM)
  + 순환 신경망(Recurrent Neural Network)
    * 순환 신경망(Recurrent Neural Network, RNN)
    * 장단기 메모리(Long Short-Term Memory, LSTM)
    * 게이트 순환 유닛(Gated Recurrent Unit, GRU)
    * 케라스의 SimpleRNN과 LSTM 이해하기
    * RNN 언어 모델(Recurrent Neural Network Language Model, RNNLM)
    * RNN을 이용한 텍스트 생성(Text Generation using RNN)
    * 문자 단위 RNN(Char RNN)



> **Django**

* 점프 투 장고
  * [장고 개발 준비](https://github.com/p-chanmin/TIL/blob/main/Django/1-00.%20%EC%9E%A5%EA%B3%A0%20%EA%B0%9C%EB%B0%9C%20%EC%A4%80%EB%B9%84.md)
  * 장고의 기본 요소



> **GitHub**

* [GitHub Markdown 수식 첨부용 사이트](https://latex.codecogs.com/)
