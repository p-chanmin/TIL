> 출처 : https://wikidocs.net/22650



# 2. Bag of Words(BoW)

Bag of Words(BoW)란 단어의 등장 순서를 고려하지 않는 빈도수 기반의 단어 표현 방법



## 2.1. Bag of Words란?

Bag of Words란 단어들의 순서는 전혀 고려하지 않고, 단어들의 출현 빈도(frequency)에만 집중하는 텍스트 데이터의 수치화 표현 방법

BoW를 만드는 과정

* 각 단어에 고유한 정수 인덱스를 부여합니다.  # 단어 집합 생성.
* 각 인덱스의 위치에 단어 토큰의 등장 횟수를 기록한 벡터를 만듭니다.  

```python
from konlpy.tag import Okt

okt = Okt()

def build_bag_of_words(document):
  # 온점 제거 및 형태소 분석
  document = document.replace('.', '')
  tokenized_document = okt.morphs(document)

  word_to_index = {}
  bow = []

  for word in tokenized_document:  
    if word not in word_to_index.keys():
      word_to_index[word] = len(word_to_index)  
      # BoW에 전부 기본값 1을 넣는다.
      bow.insert(len(word_to_index) - 1, 1)
    else:
      # 재등장하는 단어의 인덱스
      index = word_to_index.get(word)
      # 재등장한 단어는 해당하는 인덱스의 위치에 1을 더한다.
      bow[index] = bow[index] + 1

  return word_to_index, bow
```

```python
doc1 = "정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다."
vocab, bow = build_bag_of_words(doc1)
print('vocabulary :', vocab)
print('bag of words vector :', bow)
```

```
vocabulary : {'정부': 0, '가': 1, '발표': 2, '하는': 3, '물가상승률': 4, '과': 5, '소비자': 6, '느끼는': 7, '은': 8, '다르다': 9}
bag of words vector : [1, 2, 1, 1, 2, 1, 1, 1, 1, 1]
```



## 2.**2. Bag of Words의 다른 예제들**

BoW는 종종 여러 문서의 단어 집합을 합친 뒤에, 해당 단어 집합에 대한 각 문서의 BoW를 구하기도 한다.

BoW는 각 단어가 등장한 횟수를 수치화하는 텍스트 표현 방법이므로 주로 어떤 단어가 얼마나 등장했는지를 기준으로 문서가 어떤 성격의 문서인지를 판단하는 작업에 쓰인다.

즉, 분류 문제나 여러 문서 간의 유사도를 구하는 문제에 주로 쓰인다.



## 2.**3. CountVectorizer 클래스로 BoW 만들기**

사이킷 런에서는 단어의 빈도를 Count하여 Vector로 만드는 CountVectorizer 클래스를 지원

이를 이용하면 영어에 대해서는 손쉽게 BoW를 만들 수 있다.

```python
from sklearn.feature_extraction.text import CountVectorizer

corpus = ['you know I want your love. because I love you.']
vector = CountVectorizer()

# 코퍼스로부터 각 단어의 빈도수를 기록
print('bag of words vector :', vector.fit_transform(corpus).toarray()) 

# 각 단어의 인덱스가 어떻게 부여되었는지를 출력
print('vocabulary :',vector.vocabulary_)
```

```
bag of words vector : [[1 1 2 1 2 1]]
vocabulary : {'you': 4, 'know': 1, 'want': 3, 'your': 5, 'love': 2, 'because': 0}
```

CountVectorizer가 기본적으로 길이가 2이상인 문자에 대해서만 토큰으로 인식하기 때문

CountVectorizer는 단지 띄어쓰기만을 기준으로 단어를 자르는 낮은 수준의 토큰화를 진행하고 BoW를 만든다

영어의 경우 띄어쓰기만으로 토큰화가 수행되기 때문에 문제가 없지만 한국어에 CountVectorizer를 적용하면, 조사 등의 이유로 제대로 BoW가 만들어지지 않음을 의미



## 2.4. **불용어를 제거한 BoW 만들기**

불용어는 자연어 처리에서 별로 의미를 갖지 않는 단어들이다.

BoW를 사용한다는 것은 그 문서에서 각 단어가 얼마나 자주 등장했는지를 보겠다는 것

각 단어에 대한 빈도수를 수치화 하겠다는 것은 결국 텍스트 내에서 어떤 단어들이 중요한지를 보고싶다는 의미를 함축, 따라서 BoW를 만들때 불용어를 제거하는 일은 자연어 처리의 정확도를 높이기 위해서 선택할 수 있는 전처리 기법

영어의 BoW를 만들기 위해 사용하는 CountVectorizer는 불용어를 지정하면, 불용어는 제외하고 BoW를 만들 수 있도록 불용어 제거 기능을 지원

```python
from sklearn.feature_extraction.text import CountVectorizer
from nltk.corpus import stopwords
```

* **사용자가 직접 정의한 불용어 사용**

  ```python
  text = ["Family is not an important thing. It's everything."]
  vect = CountVectorizer(stop_words=["the", "a", "an", "is", "not"])
  print('bag of words vector :',vect.fit_transform(text).toarray())
  print('vocabulary :',vect.vocabulary_)
  ```

  ```
  bag of words vector : [[1 1 1 1 1]]
  vocabulary : {'family': 1, 'important': 2, 'thing': 4, 'it': 3, 'everything': 0}
  ```

* **CountVectorizer에서 제공하는 자체 불용어 사용**

  ```python
  text = ["Family is not an important thing. It's everything."]
  vect = CountVectorizer(stop_words="english")
  print('bag of words vector :',vect.fit_transform(text).toarray())
  print('vocabulary :',vect.vocabulary_)
  ```

  ```
  bag of words vector : [[1 1 1]]
  vocabulary : {'family': 0, 'important': 1, 'thing': 2}
  ```

* **NLTK에서 지원하는 불용어 사용**

  ```python
  text = ["Family is not an important thing. It's everything."]
  vect = CountVectorizer(stop_words=stopwords.words("english"))
  print('bag of words vector :',vect.fit_transform(text).toarray()) 
  print('vocabulary :',vect.vocabulary_)
  ```

  ```
  bag of words vector : [[1 1 1 1]]
  vocabulary : {'family': 1, 'important': 2, 'thing': 3, 'everything': 0}
  ```

  

