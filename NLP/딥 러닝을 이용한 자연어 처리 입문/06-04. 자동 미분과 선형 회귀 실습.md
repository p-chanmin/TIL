> 출처 : https://wikidocs.net/111472



# 4. 자동 미분과 선형 회귀 실습

선형 회귀를 텐서플로우와 케라스를 통해 구현



## 4.**1. 자동 미분**

```python
import tensorflow as tf
```

tape_gradient()는 자동 미분 기능을 수행

<img src="https://latex.codecogs.com/svg.image?2w^2&plus;5" title="2w^2+5" />

위 식을 세우고 미분

```python
w = tf.Variable(2.)

def f(w):
  y = w**2
  z = 2*y + 5
  return z
```

gradients를 출력하면 w에 대해 미분한 값이 저장된 것을 확인

```python
with tf.GradientTape() as tape:
  z = f(w)

gradients = tape.gradient(z, [w])
print(gradients)
```

```
[<tf.Tensor: shape=(), dtype=float32, numpy=8.0>]
```



## 4.**2. 자동 미분을 이용한 선형 회귀 구현**

우선 가중치 변수 w와 b를 선언

임의의 값인 4와 1로 초기화

```python
# 학습될 가중치 변수를 선언
w = tf.Variable(4.0)
b = tf.Variable(1.0)
```

가설을 함수로서 정의

```python
@tf.function
def hypothesis(x):
  return w*x + b
```

평균 제곱 오차를 손실 함수로서 정의

```python
@tf.function
def mse_loss(y_pred, y):
  # 두 개의 차이값을 제곱을 해서 평균을 취한다.
  return tf.reduce_mean(tf.square(y_pred - y))
```

사용할 데이터는 x와 y가 약 10배의 차이를 가지는 데이터

```python
x = [1, 2, 3, 4, 5, 6, 7, 8, 9] # 공부하는 시간
y = [11, 22, 33, 44, 53, 66, 77, 87, 95] # 각 공부하는 시간에 맵핑되는 성적
```

옵티마이저는 경사 하강법을 사용하되, 학습률(learning rate)는 0.01을 사용

```python
optimizer = tf.optimizers.SGD(0.01)
```

약 300번에 걸쳐서 경사 하강법을 수행

```python
for i in range(301):
  with tf.GradientTape() as tape:
    # 현재 파라미터에 기반한 입력 x에 대한 예측값을 y_pred
    y_pred = hypothesis(x)

    # 평균 제곱 오차를 계산
    cost = mse_loss(y_pred, y)

  # 손실 함수에 대한 파라미터의 미분값 계산
  gradients = tape.gradient(cost, [w, b])

  # 파라미터 업데이트
  optimizer.apply_gradients(zip(gradients, [w, b]))

  if i % 10 == 0:
    print("epoch : {:3} | w의 값 : {:5.4f} | b의 값 : {:5.4} | cost : {:5.6f}".format(i, w.numpy(), b.numpy(), cost))
```

```
epoch :   0 | w의 값 : 8.2133 | b의 값 : 1.664 | cost : 1402.555542
epoch :  10 | w의 값 : 10.4971 | b의 값 : 1.977 | cost : 1.351182
epoch :  20 | w의 값 : 10.5047 | b의 값 :  1.93 | cost : 1.328165
epoch :  30 | w의 값 : 10.5119 | b의 값 : 1.884 | cost : 1.306967
epoch :  40 | w의 값 : 10.5188 | b의 값 : 1.841 | cost : 1.287436
epoch :  50 | w의 값 : 10.5254 | b의 값 : 1.799 | cost : 1.269459
epoch :  60 | w의 값 : 10.5318 | b의 값 : 1.759 | cost : 1.252898
epoch :  70 | w의 값 : 10.5379 | b의 값 : 1.721 | cost : 1.237644
epoch :  80 | w의 값 : 10.5438 | b의 값 : 1.684 | cost : 1.223598
epoch :  90 | w의 값 : 10.5494 | b의 값 : 1.648 | cost : 1.210658
epoch : 100 | w의 값 : 10.5548 | b의 값 : 1.614 | cost : 1.198740
epoch : 110 | w의 값 : 10.5600 | b의 값 : 1.582 | cost : 1.187767
epoch : 120 | w의 값 : 10.5650 | b의 값 :  1.55 | cost : 1.177665
epoch : 130 | w의 값 : 10.5697 | b의 값 :  1.52 | cost : 1.168354
epoch : 140 | w의 값 : 10.5743 | b의 값 : 1.492 | cost : 1.159782
epoch : 150 | w의 값 : 10.5787 | b의 값 : 1.464 | cost : 1.151890
epoch : 160 | w의 값 : 10.5829 | b의 값 : 1.437 | cost : 1.144619
epoch : 170 | w의 값 : 10.5870 | b의 값 : 1.412 | cost : 1.137924
epoch : 180 | w의 값 : 10.5909 | b의 값 : 1.387 | cost : 1.131752
epoch : 190 | w의 값 : 10.5946 | b의 값 : 1.364 | cost : 1.126073
epoch : 200 | w의 값 : 10.5982 | b의 값 : 1.341 | cost : 1.120843
epoch : 210 | w의 값 : 10.6016 | b의 값 :  1.32 | cost : 1.116026
epoch : 220 | w의 값 : 10.6049 | b의 값 : 1.299 | cost : 1.111589
epoch : 230 | w의 값 : 10.6081 | b의 값 : 1.279 | cost : 1.107504
epoch : 240 | w의 값 : 10.6111 | b의 값 :  1.26 | cost : 1.103736
epoch : 250 | w의 값 : 10.6140 | b의 값 : 1.242 | cost : 1.100273
epoch : 260 | w의 값 : 10.6168 | b의 값 : 1.224 | cost : 1.097082
epoch : 270 | w의 값 : 10.6195 | b의 값 : 1.207 | cost : 1.094143
epoch : 280 | w의 값 : 10.6221 | b의 값 : 1.191 | cost : 1.091434
epoch : 290 | w의 값 : 10.6245 | b의 값 : 1.176 | cost : 1.088940
epoch : 300 | w의 값 : 10.6269 | b의 값 : 1.161 | cost : 1.086645
```

w와 b값이 계속 업데이트 됨에 따라서 cost가 지속적으로 줄어드는 것을 확인할 수 있다.

학습된 w와 b의 값에 대해서 임의 입력을 넣었을 경우의 예측값을 확인

```python
x_test = [3.5, 5, 5.5, 6]
print(hypothesis(x_test).numpy())
```

```
[38.35479  54.295143 59.608593 64.92204 ]
```



## 4.**3. 케라스로 구현하는 선형 회귀**

케라스로 모델을 만드는 기본적인 형식은 다음과 같다. Sequential로 model이라는 이름의 모델을 만들고, 그리고 add를 통해 입력과 출력 벡터의 차원과 같은 필요한 정보들을 추가해 나간다.

첫번째 인자인 1은 출력의 차원을 정의. 일반적으로 output_dim으로 표현되는 인자.

두번째 인자인 input_dim은 입력의 차원을 정의하는데 1개의 실수 x를 가지고 하는 1개의 실수 y를 예측하는 단순 선형 회귀를 구현하는 경우에는 각각 1의 값을 가진다.

```
# 예시 코드. 실행 불가.
model = Sequential()
model.add(keras.layers.Dense(1, input_dim=1))
```

activation은 어떤 함수를 사용할 것인지를 의미하는데 선형 회귀를 사용할 경우에는 linear라고 기재한다.

옵티마이저로 기본 경사 하강법을 사용하고 싶다면, sgd라고 기재, 학습률은 0.01로 정의

손실 함수로는 평균 제곱 오차를 사용, 전체 데이터에 대한 훈련 횟수는 300

```python
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras import optimizers

x = [1, 2, 3, 4, 5, 6, 7, 8, 9] # 공부하는 시간
y = [11, 22, 33, 44, 53, 66, 77, 87, 95] # 각 공부하는 시간에 맵핑되는 성적

model = Sequential()

# 출력 y의 차원은 1. 입력 x의 차원(input_dim)은 1
# 선형 회귀이므로 activation은 'linear'
model.add(Dense(1, input_dim=1, activation='linear'))

# sgd는 경사 하강법을 의미. 학습률(learning rate, lr)은 0.01.
sgd = optimizers.SGD(lr=0.01)

# 손실 함수(Loss function)은 평균제곱오차 mse를 사용합니다.
model.compile(optimizer=sgd, loss='mse', metrics=['mse'])

# 주어진 x와 y데이터에 대해서 오차를 최소화하는 작업을 300번 시도합니다.
model.fit(x, y, epochs=300)
```

최종적으로 선택된 오차를 최소화하는 직선을 그래프

![img](https://wikidocs.net/images/page/21670/%EC%A7%81%EC%84%A0.png)