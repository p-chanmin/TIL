> 출처 : https://wikidocs.net/31698



# 4. TF-IDF(Term Frequency-Inverse Document Frequency)

DTM 내에 있는 각 단어에 대한 중요도를 계산할 수 있는 TF-IDF 가중치

TF-IDF를 사용하면, 기존의 DTM을 사용하는 것보다 보다 많은 정보를 고려하여 문서들을 비교할 수 있다.

TF-IDF가 DTM보다 항상 좋은 성능을 보장하는 것은 아니지만, 많은 경우에서 DTM보다 더 좋은 성능을 얻을 수 있다.



## 4.**1. TF-IDF(단어 빈도-역 문서 빈도, Term Frequency-Inverse Document Frequency)**

TF-IDF(Term Frequency-Inverse Document Frequency)는 단어의 빈도와 역 문서 빈도(문서의 빈도에 특정 식을 취함)를 사용하여 DTM 내의 각 단어들마다 중요한 정도를 가중치로 주는 방법

우선 DTM을 만든 후, TF-IDF 가중치를 부여

TF-IDF는 주로 문서의 유사도를 구하는 작업, 검색 시스템에서 검색 결과의 중요도를 정하는 작업, 문서 내에서 특정 단어의 중요도를 구하는 작업 등에 쓰일 수 있다.

TF-IDF는 TF와 IDF를 곱한 값을 의미

문서를 d, 단어를 t, 문서의 총 개수를 n이라고 표현할 때 TF, DF, IDF는 각각 다음과 같이 정의

* **tf(d,t) : 특정 문서 d에서의 특정 단어 t의 등장 횟수.**

* **df(t) : 특정 단어 t가 등장한 문서의 수.**

* **idf(d, t) : df(t)에 반비례하는 수.**

  <img src="https://latex.codecogs.com/svg.image?idf(d,&space;t)&space;=&space;log(\frac{n}{1&plus;df(t)})" title="idf(d, t) = log(\frac{n}{1+df(t)})" />

  IDF의 값은 기하급수적으로 커지게 되는 상황을 막기 위해 log를 사용

---

TF-IDF는 모든 문서에서 자주 등장하는 단어는 중요도가 낮다고 판단하며, 특정 문서에서만 자주 등장하는 단어는 중요도가 높다고 판단

**TF-IDF 값이 낮으면 중요도가 낮은 것이며, TF-IDF 값이 크면 중요도가 큰 것**

즉, the나 a와 같이 불용어의 경우에는 모든 문서에 자주 등장하기 마련이기 때문에 자연스럽게 불용어의 TF-IDF의 값은 다른 단어의 TF-IDF에 비해서 낮아지게 된다.

---

| -     | 과일이 | 길고 | 노란 | 먹고 | 바나나 | 사과 | 싶은 | 저는 | 좋아요 |
| :---- | :----- | :--- | :--- | :--- | :----- | :--- | :--- | :--- | :----- |
| 문서1 | 0      | 0    | 0    | 1    | 0      | 1    | 1    | 0    | 0      |
| 문서2 | 0      | 0    | 0    | 1    | 1      | 0    | 1    | 0    | 0      |
| 문서3 | 0      | 1    | 1    | 0    | 2      | 0    | 0    | 0    | 0      |
| 문서4 | 1      | 0    | 0    | 0    | 0      | 0    | 0    | 1    | 1      |

위의 예제를 가지고 TF-IDF에 대해 이해

앞서 사용한 DTM을 그대로 사용하면, 그것이 각 문서에서의 각 단어의 TF가 된다.

구해야할 것은 TF와 곱해야할 값인 IDF. 로그는 자연 로그를 사용

IDF 계산을 위해 사용하는 로그의 밑은 TF-IDF를 사용하는 사용자가 임의로 정할 수 있는데, 여기서 로그는 마치 기존의 값에 곱하여 값의 크기를 조절하는 상수의 역할

각종 프로그래밍 언어에서 패키지로 지원하는 TF-IDF의 로그는 대부분 자연 로그를 사용

| 단어   | IDF(역 문서 빈도)      |
| :----- | :--------------------- |
| 과일이 | ln(4/(1+1)) = 0.693147 |
| 길고   | ln(4/(1+1)) = 0.693147 |
| 노란   | ln(4/(1+1)) = 0.693147 |
| 먹고   | ln(4/(2+1)) = 0.287682 |
| 바나나 | ln(4/(2+1)) = 0.287682 |
| 사과   | ln(4/(1+1)) = 0.693147 |
| 싶은   | ln(4/(2+1)) = 0.287682 |
| 저는   | ln(4/(1+1)) = 0.693147 |
| 좋아요 | ln(4/(1+1)) = 0.693147 |

TF-IDF를 계산

각 단어의 TF는 DTM에서의 각 단어의 값과 같으므로, 앞서 사용한 DTM에서 단어 별로 위의 IDF값을 곱해주면 TF-IDF 값을 얻을 수 있다.

| -     | 과일이   | 길고     | 노란     | 먹고     | 바나나   | 사과     | 싶은     | 저는     | 좋아요   |
| :---- | :------- | :------- | :------- | :------- | :------- | :------- | :------- | :------- | :------- |
| 문서1 | 0        | 0        | 0        | 0.287682 | 0        | 0.693147 | 0.287682 | 0        | 0        |
| 문서2 | 0        | 0        | 0        | 0.287682 | 0.287682 | 0        | 0.287682 | 0        | 0        |
| 문서3 | 0        | 0.693147 | 0.693147 | 0        | 0.575364 | 0        | 0        | 0        | 0        |
| 문서4 | 0.693147 | 0        | 0        | 0        | 0        | 0        | 0        | 0.693147 | 0.693147 |



## 4.**2. 파이썬으로 TF-IDF 직접 구현하기**

4개의 문서를 docs에 저장

```python
import pandas as pd # 데이터프레임 사용을 위해
from math import log # IDF 계산을 위해

docs = [
  '먹고 싶은 사과',
  '먹고 싶은 바나나',
  '길고 노란 바나나 바나나',
  '저는 과일이 좋아요'
] 
vocab = list(set(w for doc in docs for w in doc.split()))
vocab.sort()
```

TF, IDF, 그리고 TF-IDF 값을 구하는 함수를 구현

```python
# 총 문서의 수
N = len(docs) 

def tf(t, d):
  return d.count(t)

def idf(t):
  df = 0
  for doc in docs:
    df += t in doc
  return log(N/(df+1))

def tfidf(t, d):
  return tf(t,d)* idf(t)
```

DTM 출력

```python
result = []

# 각 문서에 대해서 아래 연산을 반복
for i in range(N):
  result.append([])
  d = docs[i]
  for j in range(len(vocab)):
    t = vocab[j]
    result[-1].append(tf(t, d))

tf_ = pd.DataFrame(result, columns = vocab)
tf_
```

![img](https://wikidocs.net/images/page/31698/tf_.PNG)

IDF 값 출력

```python
result = []
for j in range(len(vocab)):
    t = vocab[j]
    result.append(idf(t))

idf_ = pd.DataFrame(result, index=vocab, columns=["IDF"])
idf_
```

![img](https://wikidocs.net/images/page/31698/idf_.PNG)

TF-IDF 행렬을 출력

```python
result = []
for i in range(N):
  result.append([])
  d = docs[i]
  for j in range(len(vocab)):
    t = vocab[j]
    result[-1].append(tfidf(t,d))

tfidf_ = pd.DataFrame(result, columns = vocab)
tfidf_
```

![img](https://wikidocs.net/images/page/31698/tfidf_.PNG)



## 4.**3. 사이킷런을 이용한 DTM과 TF-IDF 실습**

사이킷런을 통해 DTM과 TF-IDF을 만들 수 있다.

CountVectorizer를 사용하면 DTM을 만들 수 있다.

```python
from sklearn.feature_extraction.text import CountVectorizer

corpus = [
    'you know I want your love',
    'I like you',
    'what should I do ',    
]

vector = CountVectorizer()

# 코퍼스로부터 각 단어의 빈도수를 기록
print(vector.fit_transform(corpus).toarray())

# 각 단어와 맵핑된 인덱스 출력
print(vector.vocabulary_)
```

```
[[0 1 0 1 0 1 0 1 1]
 [0 0 1 0 0 0 0 1 0]
 [1 0 0 0 1 0 1 0 0]]
{'you': 7, 'know': 1, 'want': 5, 'your': 8, 'love': 3, 'like': 2, 'what': 6, 'should': 4, 'do': 0}
```

사이킷런은 TF-IDF를 자동 계산해주는 TfidfVectorizer를 제공

사이킷런의 TF-IDF는 보편적인 TF-IDF 기본 식에서 조정된 식을 사용 (IDF의 로그항의 분자에 1을 더해주며, 로그항에 1을 더해주고, TF-IDF에 L2 정규화라는 방법으로 값을 조정)

```python
from sklearn.feature_extraction.text import TfidfVectorizer

corpus = [
    'you know I want your love',
    'I like you',
    'what should I do ',    
]

tfidfv = TfidfVectorizer().fit(corpus)
print(tfidfv.transform(corpus).toarray())
print(tfidfv.vocabulary_)
```

```
[[0.         0.46735098 0.         0.46735098 0.         0.46735098 0.         0.35543247 0.46735098]
 [0.         0.         0.79596054 0.         0.         0.         0.         0.60534851 0.        ]
 [0.57735027 0.         0.         0.         0.57735027 0.         0.57735027 0.         0.        ]]
{'you': 7, 'know': 1, 'want': 5, 'your': 8, 'love': 3, 'like': 2, 'what': 6, 'should': 4, 'do': 0}
```

