> 출처 : https://wikidocs.net/21692



# 3. N-gram 언어 모델(N-gram Language Model)

n-gram 언어 모델은 여전히 카운트에 기반한 통계적 접근을 사용하고 있으므로 SLM의 일종

이전에 등장한 모든 단어를 고려하는 것이 아니라 일부 단어만 고려하는 접근 방법을 사용

이때 일부 단어를 몇 개 보느냐를 결정하는데 이것이 n-gram에서의 n이 가지는 의미



## 3.1. 코퍼스에서 카운트하지 못하는 경우의 감소

SLM의 한계는 훈련 코퍼스에 확률을 계산하고 싶은 문장이나 단어가 없을 수 있다는 점

확률을 계산하고 싶은 문장이 길어질수록 갖고있는 코퍼스에서 그 문장이 존재하지 않을 가능성이 높다.

참고하는 단어들을 줄이면 카운트를 할 수 있을 가능성이 높일 수 있다.

<img src="https://latex.codecogs.com/svg.image?P(\text{is}|\text{An&space;adorable&space;little&space;boy})&space;\approx\&space;P(\text{is}|\text{boy})" title="P(\text{is}|\text{An adorable little boy}) \approx\ P(\text{is}|\text{boy})" />

<img src="https://latex.codecogs.com/svg.image?P(\text{is}|\text{An&space;adorable&space;little&space;boy})&space;\approx\&space;P(\text{is}|\text{little&space;boy})" title="P(\text{is}|\text{An adorable little boy}) \approx\ P(\text{is}|\text{little boy})" />

앞에서는 An adorable little boy가 나왔을 때 is가 나올 확률을 구하기 위해서는 An adorable little boy가 나온 횟수와 An adorable little boy is가 나온 횟수를 카운트해야만 했다.

N-gram 언어 모델에서는 단어의 확률을 구하고자 기준 단어의 앞 단어를 전부 포함해서 카운트하는 것이 아니라, 앞 단어 중 임의의 개수만 포함해서 카운트하여 근사하는 것

이렇게 하면 갖고 있는 코퍼스에서 해당 단어의 시퀀스를 카운트할 확률이 높아진다.



## 3.2. **N-gram**

이때 임의의 개수를 정하기 위한 기준을 위해 사용하는 것이 n-gram이다.

n-gram은 n개의 연속적인 단어 나열을 의미

갖고 있는 코퍼스에서 n개의 단어 뭉치 단위로 끊어서 이를 하나의 토큰으로 간주

* 유니그램(unigram) : n = 1 일때
* 바이그램(bigram) : n = 2 일때
* 트라이그램(trigram) : n = 3 일때
* n-gram : n = 4 이상일 때는 gram 앞에 그대로 숫자를 붙여서 명명(4-gram, 5-gram, 6-gram,...)

n-gram을 통한 언어 모델에서는 다음에 나올 단어의 예측은 오직 n-1개의 단어에만 의존

![img](https://wikidocs.net/images/page/21692/n-gram.PNG)

<img src="https://latex.codecogs.com/svg.image?P(w|\text{boy&space;is&space;spreading})&space;=&space;\frac{\text{count(boy&space;is&space;spreading}\&space;w)}{\text{count(boy&space;is&space;spreading)}}" title="P(w|\text{boy is spreading}) = \frac{\text{count(boy is spreading}\ w)}{\text{count(boy is spreading)}}" />

만약 갖고있는 코퍼스에서 boy is spreading가 1,000번 등장했다고 하자.

boy is spreading insults가 500번 등장했으며, boy is spreading smiles가 200번 등장했다고 한다면 boy is spreading 다음에 insults가 등장할 확률은 50%이며, smiles가 등장할 확률은 20%이다.

확률적 선택에 따라 insults가 더 맞다고 판단하게 된다.



## 3.3. **N-gram Language Model의 한계**

n-gram은 뒤의 단어 몇 개만 보다 보니 의도하고 싶은 대로 문장을 끝맺음하지 못하는 경우가 생긴다는 점

문장을 읽다 보면 앞 부분과 뒷부분의 문맥이 전혀 연결 안 되는 경우도 생길 수 있다.

전체 문장을 고려한 언어 모델보다는 정확도가 떨어질 수밖에 없다.

* **희소 문제(Sparsity Problem)**

  문장에 존재하는 앞에 나온 단어를 모두 보는 것보다 일부 단어만을 보는 것으로 현실적으로 코퍼스에서 카운트 할 수 있는 확률을 높일 수는 있었지만, n-gram 언어 모델도 여전히 n-gram에 대한 희소 문제가 존재

* **n을 선택하는 것은 trade-off 문제**

  앞에서 몇 개의 단어를 볼지 n을 정하는 것은 trade-off가 존재

  n을 크게 선택하면 실제 훈련 코퍼스에서 해당 n-gram을 카운트할 수 있는 확률은 적어지므로 희소 문제는 점점 심각해지며, n이 커질수록 모델 사이즈가 커진다.

  n을 작게 선택하면 훈련 코퍼스에서 카운트는 잘 되겠지만 근사의 정확도는 현실의 확률분포와 멀어진다.

  trade-off 문제로 인해 정확도를 높이려면 **n은 최대 5를 넘게 잡아서는 안 된다고 권장**



## 3.4. **적용 분야(Domain)에 맞는 코퍼스의 수집**

어떤 분야인지, 어떤 어플리케이션인지에 따라서 특정 단어들의 확률 분포는 다르다.

훈련에 사용된 도메인 코퍼스가 무엇이냐에 따라서 성능이 비약적으로 달라진다.



## 3.5. **인공 신경망을 이용한 언어 모델(Neural Network Based Language Model)**

N-gram Language Model의 한계점을 극복하기위해 분모, 분자에 숫자를 더해서 카운트했을 때 0이 되는 것을 방지하는 등의 여러 일반화(generalization) 방법들이 존재

그럼에도 본질적으로 n-gram 언어 모델에 대한 취약점을 완전히 해결하지는 못하였고, 이를 위한 대안으로 N-gram Language Model보다 대체적으로 성능이 우수한 **인공 신경망을 이용한 언어 모델**이 많이 사용되고 있다.